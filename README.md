# [CUDA Learning]

Date Created: July 12, 2024 9:40 AM
Status: ç ”ç©¶

# CUDA Intro-tutorial

> ä¸»è¦å‚è€ƒäº†PaddleJITLabçš„Introææ–™ï¼š[CUDATutorial | Notebook (keter.top)](https://cuda.keter.top/)ï¼Œä½œè€…è¿˜æœ‰ä¸€äº›å…¶ä»–çš„Blog: [Aurelius84 - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/CocoML)
> 

### 0. Setup Environment

```jsx
(opensora) zhaotianchen@is-c7bbldidqx6mb3fj-devmachine-0:~$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Tue_Aug_15_22:02:13_PDT_2023
Cuda compilation tools, release 12.2, V12.2.140
Build cuda_12.2.r12.2/compiler.33191640_0

---
(opensora) zhaotianchen@is-c7bbldidqx6mb3fj-devmachine-0:~$ g++ --version
g++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

- æµ‹è¯•å‘½ä»¤æ”¾åœ¨infini-ztc-1ï¼› `~/project/cuda_learning/`

```jsx
nvcc hello_world.cu -o hello_world
```

### 1. Simple VecSum Example

- CPU Example: `vec_sum_cpu.cpp`
    - åˆ†é…å†…å­˜  `static_cast<float*>(malloc(mem_size))`
        - æ„å‘³ç€é¦–å…ˆä½¿ç”¨`malloc`åˆ†é…å†…å­˜ï¼Œç„¶åå°†è¿”å›çš„`void*`æŒ‡é’ˆè½¬æ¢ä¸º`float*`ç±»å‹ï¼Œä»¥ä¾¿å¯ä»¥å°†å…¶ä½œä¸ºæµ®ç‚¹æ•°æ•°ç»„æ¥ä½¿ç”¨
        - **static_cast<float*>**: è¿™æ˜¯C++ä¸­çš„ç±»å‹è½¬æ¢æ“ä½œç¬¦ï¼Œç”¨äºå°†ä¸€ä¸ªæŒ‡é’ˆè½¬æ¢æˆå¦ä¸€ç§ç±»å‹çš„æŒ‡é’ˆã€‚`static_cast`ä¸ä¼šè¿›è¡Œä»»ä½•ç±»å‹çš„æ£€æŸ¥æˆ–è½¬æ¢ï¼Œå®ƒåªæ˜¯å‘Šè¯‰ç¼–è¯‘å™¨å°†ä¸€ä¸ªæŒ‡é’ˆçš„ç±»å‹è½¬æ¢ä¸ºå¦ä¸€ä¸ªæŒ‡é’ˆçš„ç±»å‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒå°†`malloc`è¿”å›çš„`void*`æŒ‡é’ˆè½¬æ¢ä¸º`float*`ç±»å‹ï¼Œå³æŒ‡å‘`float`çš„æŒ‡é’ˆã€‚

```jsx
g++ vec_sum_cpu.cpp -o vec_sum_cpu
```

- GPU Example: `vec_sum_gpu`
    - GPUä¸Šçš„å‡½æ•°æ·»åŠ  `__**global__**`ä¿®é¥°ç¬¦å·ï¼Œæ ‡è¯†è¯¥functionæ˜¯åœ¨Deviceä¸Šçš„
    - `<<<M,T>>>` åœ¨Hostç«¯å¯åŠ¨CUDAç¨‹åºçš„å½¢å¼ï¼ŒMè¡¨ç¤ºä¸€ä¸ªgridçš„blockæ•°ç›®ï¼ŒTè¡¨ç¤ºä¸€ä¸ªblokcå¹¶è¡Œçš„threadæ•°ç›®ï¼š
        - `add_kernel<<<1, 1>>>(cuda_x, cuda_y, cuda_out, N);`
    - `cudaMalloc((*void***)&cuda_x, mem_size);`
        - å¯¹åº”æ–‡æ¡£ä¸­çš„è¯´æ˜ï¼š`__host____device__[cudaError_t](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6)Â [cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356)Â (void**Â devPtr, size_tÂ sizeÂ )`   Allocate memory on the device.
    - **å†…å­˜é‡Šæ”¾**: ä½¿ç”¨ `cudaMalloc` åˆ†é…çš„å†…å­˜éœ€è¦ä½¿ç”¨ `cudaFree` å‡½æ•°æ¥é‡Šæ”¾ï¼Œä»¥é¿å…**å†…å­˜æ³„æ¼**ã€‚

### 2. NVProf(Nsys) ç®€å•Profile Kernelæ€§èƒ½

- ä¼¼ä¹åœ¨æœ€æ–°çš„ç‰ˆæœ¬nvprofå·²ç»ä¸æ”¯æŒäº†ï¼Œå¯¹äºcompute capabilityè¾ƒé«˜çš„æ–°æœºå™¨â€¦

```jsx
nvprof ./vec_sum_gpu
```

```jsx

======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.
                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.
                  Refer https://developer.nvidia.com/tools-overview for more details.
```

- æ”¹ç”¨nsysï¼Œä¹Ÿæ”¯æŒå‘½ä»¤è¡Œæ‰§è¡Œï¼Œèƒ½æ¶µç›–ä¹‹å‰nvprofçš„åŠŸèƒ½ï¼š
    - åœ¨ `~/project/profiling`ä¸­æœ‰ä¸‹è½½å¥½çš„Nsightçš„dpkgï¼Œå®‰è£…åå¯ç”¨nsysè°ƒç”¨
    - æ›´å¤šçš„å‘½ä»¤è¡ŒåŠŸèƒ½  `nsys profile â€”help`
        - [User Guide â€” nsight-systems 2024.4 documentation (nvidia.com)](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)
    - ç›®å‰çš„profileæ–¹å¼  `nsys profile â€”stats=true ./vec_sum_gpu`
        - ä¼¼ä¹nsysæä¾›äº†ä¸€ç§å…¼å®¹è€ç‰ˆæœ¬nvprof commandçš„å­å‘½ä»¤ `nsys nvprof ./vec_sum_gpu` å¯ä»¥ç»™å‡ºç±»ä¼¼çš„ç»“æœã€‚
        - cuda apiçš„è°ƒç”¨å¼€é”€ï¼šç»å¤§å¤šæ•°æ—¶é—´åœ¨ç­‰å€™cudaMemCpy & cudaMealloc
    
    ```jsx
    [5/8] Executing 'cuda_api_sum' stats report
    
     Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)    Max (ns)     StdDev (ns)            Name
     --------  ---------------  ---------  -------------  -----------  ---------  -----------  -------------  ----------------------
         82.3      732,084,456          3  244,028,152.0  3,793,482.0  3,721,923  724,569,051  416,160,627.6  cudaMemcpy
         17.6      156,475,117          3   52,158,372.3    176,494.0    149,619  156,149,004   90,058,529.8  cudaMalloc
          0.1          452,890          3      150,963.3    108,418.0    102,467      242,005       78,900.5  cudaFree
          0.0          204,640          1      204,640.0    204,640.0    204,640      204,640            0.0  cudaLaunchKernel
          0.0           17,775          1       17,775.0     17,775.0     17,775       17,775            0.0  cudaDeviceSynchronize
          0.0            2,668          1        2,668.0      2,668.0      2,668        2,668            0.0  cuModuleGetLoadingMode
    ```
    
    - cuda Kernelçš„å¼€é”€
    
    ```jsx
    [6/8] Executing 'cuda_gpu_kern_sum' stats report
     Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)                     Name
     --------  ---------------  ---------  -------------  -------------  -----------  -----------  -----------  ------------------------------------------
        100.0      704,725,249          1  704,725,249.0  704,725,249.0  704,725,249  704,725,249          0.0  add_kernel(float *, float *, float *, int)
    
    [7/8] Executing 'cuda_gpu_mem_time_sum' stats report
     Time (%)  Total Time (ns)  Count    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)           Operation
     --------  ---------------  -----  ------------  ------------  ----------  ----------  -----------  ----------------------------
         72.4       19,244,480      1  19,244,480.0  19,244,480.0  19,244,480  19,244,480          0.0  [CUDA memcpy Device-to-Host]
         27.6        7,342,076      2   3,671,038.0   3,671,038.0   3,644,622   3,697,454     37,357.9  [CUDA memcpy Host-to-Device]
    ```
    
    - Memory(time/size)
    
    ```jsx
    [8/8] Executing 'cuda_gpu_mem_size_sum' stats report
    
     Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation
     ----------  -----  --------  --------  --------  --------  -----------  ----------------------------
         80.000      2    40.000    40.000    40.000    40.000        0.000  [CUDA memcpy Host-to-Device]
         40.000      1    40.000    40.000    40.000    40.000        0.000  [CUDA memcpy Device-to-Host]
    
    ```
    

### 3. å¤šçº¿ç¨‹æ ·ä¾‹ï¼šå¹¶è¡Œè®¡ç®—å‘é‡å’Œ

- å°†ä¸Šé¢ `<<<1,1>>>`çš„kernelæ”¹ä¸ºå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼Œå¹¶è¡Œå¤šä¸ªthreadè¿›è¡Œè®¡ç®—
    - æ”¹å†™åœ¨äº† `vec_sum_gpu_T256.cu` ä¸­
- CUDAæä¾›äº†**å†…å»ºå˜é‡**æ¥è®¿é—®æ¯ä¸ªthreadçš„ç›¸å…³ä¿¡æ¯ï¼ˆåœ¨æŸä¸ªGridä¸­ç´¢å¼•çº¿ç¨‹ï¼‰ï¼š
    - `threadIdx.x`: æŒ‡æ­¤çº¿ç¨‹åœ¨`thread block`ä¸­çš„ä¸‹æ ‡ä½ç½®
    - `blockDim.x`: æŒ‡ä¸€ä¸ª`thread block`ä¸­çš„çº¿ç¨‹æ•°
- è‹¥è¦å°†æŸä¸ªè®¡ç®—workloadï¼Œåˆ‡åˆ†åˆ°256ä¸ªçº¿ç¨‹ä¸­è¿›è¡Œè®¡ç®—ï¼Œå¯è®¾ç½®strideï¼š
    - è¯»â€å½“å‰çš„threadâ€œåœ¨blockä¸­çš„ä¸‹æ ‡ä½ç½®ï¼Œä»¥blockä¸­çš„threadä¸ªæ•°ä¸ºstrideï¼ˆåœ¨å¤–é¢add_kernelçš„ä¹‹åæ”¹æˆ `<<<1,256>>>`ï¼Œè°ƒç”¨ä¸€ä¸ªblockï¼Œblockä¸­æœ‰256ä¸ªthreadï¼‰
    
    ```jsx
    __global__ void add_kernel(float *x, float *y, float *out, int n){
        int index = threadIdx.x;  // å½“å‰çº¿ç¨‹æ‰€åœ¨çš„ä¸‹æ ‡ä½ç½®
        int stride = blockDim.x;  // æ­¤æ ·ä¾‹ä¸­ä¸º 256ï¼Œç”±<<<1, 256>>>è‡ªåŠ¨ä¼ é€’è¿‡æ¥
    
        for (int i = index; i < n; i += stride) {
            out[i] = x[i] + y[i];
        }
    }
    ```
    
    ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled.png)
    

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%201.png)

- å»¶è¿Ÿ(256 threads): 22ms

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%202.png)

- å»¶è¿Ÿï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰ï¼š711 ms

---

- ä¸ä»…ç”¨æ›´å¤šçš„threadï¼Œä¹ŸåŒæ—¶è°ƒç”¨æ›´å¤šçš„blockè¿›è¡Œè®¡ç®—
    - ä¿®æ”¹add_kernelè®©æ¯ä¸ªthreadåªæ˜¯è¿›è¡Œä¸€ä¸ªå°å°çš„åŠ æ³•
    
    ```jsx
      3 __global__ void add_kernel(float *x, float *y, float *out, int n){
      4     int tid = blockIdx.x * blockDim.x + threadIdx.x;
      5
      6      if (tid < n) {  // due to N is not divisible by block_size, some blocks need to run empty
      7         out[tid] = x[tid] + y[tid];
      8     }
      9 }
    ```
    
    - ä¿®æ”¹Kernel Launchingçš„é…ç½®
    
    ```jsx
     37     int block_size = 256;
     38     int grid_size = (N+block_size)/block_size;
     39     add_kernel<<<grid_size, block_size>>>(cuda_x, cuda_y, cuda_out, N);
    ```
    

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%203.png)

### 4. å¤šçº¿ç¨‹åŸç†ï¼š

- GPU ä¸Šä¸€èˆ¬åŒ…å«å¾ˆå¤šæµå¼å¤„ç†å™¨ SMï¼ˆStream Processorï¼Œæ¯”å¦‚V100å°±æœ‰80ä¸ªSMï¼ŒSMä¸­åŒ…å«äº†CUDA Coreï¼‰ï¼ŒSMå¯ä»¥çœ‹åšæ˜¯**åŸºæœ¬è®¡ç®—å•å…ƒ**ï¼Œå…¶ä¸­åˆ‡åˆ†æˆäº†è‹¥å¹²ä¸ªGridï¼Œæ¯ä¸ªGridä¸­åŒ…å«äº†è‹¥å¹²ä¸ªçº¿ç¨‹å—ï¼ˆBlockï¼Œæ¯”å¦‚65536ï¼‰ï¼Œæ¯ä¸ªçº¿ç¨‹å—åŒ…å«äº†è‹¥å¹²çº¿ç¨‹ï¼ˆThreadï¼Œå¦‚512ï¼‰ã€‚
    - Threadè®¡ç®—çš„åŸºæœ¬å•ä½ï¼Œä¸€ä¸ªCUDA Kernelï¼ˆworkloadï¼‰å¯ä»¥è¢«å¤šä¸ªThreadï¼ˆå®é™…çš„ç¡¬ä»¶å•å…ƒç®—åŠ›æŠ½è±¡ï¼‰æ¥æ‰§è¡Œ
    - Blockï¼šç”±å¤šä¸ªThreadç»„æˆï¼ŒåŒä¸€ä¸ªblockä¸­çš„threadså¯ä»¥**äº’ç›¸åŒæ­¥**ï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®Shared Memory
    - Gridï¼šå¤šä¸ªBlockså¯ä»¥ç»„æˆä¸€ä¸ªGrid
    - **æ³¨æ„ï¼š**Blockå’ŒThreadsçš„æ’å¸ƒæ–¹å¼å¯ä»¥æ˜¯1-Dï¼Œ2-Dï¼Œ3-Dçš„
- çº¿ç¨‹å”¯ä¸€æ ‡è¯†ç¼–å·ï¼Œè®¡ç®—æ–¹å¼å’Œblock/threadçš„dimensionæ•°ç›®æœ‰å…³
    - æ¯”å¦‚Gridä¸€ç»´ï¼ŒBlockä¸ºäºŒç»´ï¼š
        
        ```jsx
        int threadId = blockIdx.x * blockDim.x * blockDim.y + 
                      threadIdx.y * blockDim.x + threadIdx.x;  
        ```
        
- Warpï¼šç”¨æ¥æ‰§è¡ŒåŒä¸€ä¸ªæŒ‡ä»¤çš„ä¸€ç»„threadï¼ˆSIMDï¼‰ï¼Œå…¸å‹å¤§å°ä¸º32
- ç»™äº†ä¸€ä¸ªæ‰“å°çº¿ç¨‹IDçš„exampleæ¥äº†è§£æœºåˆ¶ï¼š  `./print_id.cu`
    - å‡†å¤‡äº†128ä¸ªthread
    - è®¾ç½®warp_sizeä¸º32ï¼Œnum_threadsä¸º64ï¼Œnum_blocksä¸º2
    - é¢„å…ˆmallocæ¯ä¸ªarrayæœ‰128ä¸ªintå˜é‡
    - åœ¨where_is_my_idä¸­è¯»å–å†…å»ºå˜é‡çš„å€¼ï¼ˆthreadIdx, blockIdxï¼‰ï¼Œèµ‹å€¼ç»™warpä¸calc_threadç­‰æ•°ç»„ï¼Œä¸¢å›æ¥
    - **æ‰“å°å‡ºæ¥çš„ç»“æœå¯çŸ¥ï¼š**
        - block â†’ warps â†’ threads
- CUDAçš„å±‚æ¬¡ç»“æ„ï¼š
    - 3å±‚ï¼šGrid â†’ Block â†’ Thread (Warpæ˜¯ä¸€ä¸ªè½¯ä»¶æ¦‚å¿µï¼Œå¯ä»¥åœ¨çº¿ç¨‹è°ƒåº¦çš„è¿‡ç¨‹ä¸­åŠ¨æ€ç»„æˆæˆæ–°çš„thread)ï¼Œä¸€ä¸ª3ç»´çš„Exampleï¼š
        
        ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%204.png)
        
    - åœ¨æ¯æ¬¡è°ƒç”¨CUDA Kernelçš„æ—¶å€™ï¼Œéƒ½ä¼šå®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„Gridï¼Œå…¶ä¸­Blockç­‰sizeçš„å¤§å°å¯ä»¥ç”¨blockDimçš„å˜é‡è¿›è¡Œé…ç½®ã€‚
    
    ```jsx
    #define CEIL_DIV(M, N) (((M) + (N)-1) / (N))  // M+N/Nè¡¨ç¤ºCeilï¼Œåˆ†å­ä¸Šé¢å¤–-1
    
    dim3 gridDim(CEIL_DIV(M, 32), CEIL_DIV(N, 32));  // CEIL_DIV å‘ä¸Šå–æ•´çš„é™¤æ³•
    // 32 * 32 = 1024 thread per block
    dim3 blockDim(32, 32);
    ```
    

### 5.1  ç®€å•çŸ©é˜µä¹˜æ³•Exampleï¼š

- ä»¥ä¸‹æ˜¯è¿›è¡Œä¸€ä¸ªæœ´ç´ çš„GEMMå®ç°çš„é…ç½®æ–¹å¼  `simple_matmul.cu`
    - çŸ©é˜µ $A \in [M,K]$
    - çŸ©é˜µ $B \in [K,N]$
    - CPUçš„ä¸‰å±‚å¾ªç¯è®¡ç®—ï¼Œä½œä¸ºReferenceï¼š `sgemm_naive_cpu`
        - M,N,Kå¾ªç¯ä¸‰è½®; A[m,k]*B[k,n]
            - åœ¨Kç»´åº¦ä¸Šreduce sumèµ·æ¥ï¼Œèµ‹å€¼ç»™è¾“å‡ºçŸ©é˜µçš„[m,n]
    - ä¸€ä¸ªæœ´ç´ çš„gemm kernel  `sgemm_naive_kernel`
        - å¦‚æœ(m<M, n<N)ï¼Œåœ¨Mï¼ŒNç»´åº¦ä¸Šparallelï¼Œä¸€å±‚å¾ªç¯K
            - äºŒç»´ï¼šblock*thread = ((M,32),32) = M

### 5.2  ä¼˜åŒ–çŸ©é˜µä¹˜æ³•Example-1ï¼ˆä½¿ç”¨SharedMemoryï¼‰ï¼š

> æ›´å¤šæœ‰å…³Shared Memoryçš„èµ„æºï¼š[Using Shared Memory in CUDA C/C++ | NVIDIA Technical Blog](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)
> 
- **To-Learnï¼š**å¦‚ä½•ä½¿ç”¨å…±äº«å†…å­˜å—ï¼ˆSMEMï¼‰ï¼Œå»å°†å…¨å±€å†…å­˜ä¸­å±€éƒ¨çš„ä¸€éƒ¨åˆ†åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­è¿›è¡Œè®¡ç®—ï¼Œä»¥**å‡å°‘å¯¹å…¨å±€å†…å­˜çš„è®¿é—®æ¬¡æ•°ï¼Œç”¨Shared Memoryçš„è®¿é—®æ¥æ›¿ä»£**ã€‚
    - æ¯ä¸ªSMéƒ½æœ‰ä¸€å—å•ç‹¬çš„å…±äº«å†…å­˜ï¼›
    - å…±äº«å†…å­˜å—çš„å¤§å°æ˜¯å¯ä»¥é…ç½®çš„ï¼Œä¸L1ç¼“å­˜å…±ç”¨ã€‚
- **æ€è·¯ï¼šå°†å±€éƒ¨çš„32x32çš„æ•°æ®ç»™Cacheåˆ°Shared Memoryä¸­ï¼Œåœ¨ä¸€ä¸ªBlockå†…éƒ¨è¿›è¡Œå¹¶è¡Œè®¡ç®—**
    - Load Data: å°†32x32çš„æ•°æ®ä»Global Memoryè¯»å–åˆ°Shared memoryå½“ä¸­
    - åœ¨å½“å‰ç¼“å­˜å—ä¸Šè¿›è¡Œç‚¹ç§¯ï¼ˆä¹˜æ³•å¹¶ç´¯åŠ ï¼‰
    - æ¨åŠ¨æŒ‡é’ˆå–ä¸‹ä¸€ä¸ªcache block

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%205.png)

- Code Read:
    - å¯¹Kernelå£°æ˜ä¸€ä¸ªå…¨å±€å˜é‡
        - åœ¨ CUDA çš„ `__global__` å‡½æ•°ä¸­ä½¿ç”¨ `template` è€Œæ˜¯ä¸ºäº†åœ¨å®šä¹‰åœ¨ç¼–è¯‘æ—¶å‚æ•°åŒ–å†…æ ¸å‡½æ•°çš„å¸¸é‡å€¼
    
    ```jsx
    template <const int BLOCKSIZE>
    ```
    
    - å£°æ˜Shared Memoryï¼š
        - `__shared__` å…³é”®å­—
    
    ```jsx
        // allocate shared memory for the input and output submatrices
        __shared__ float A_shared[BLOCKSIZE * BLOCKSIZE];
        __shared__ float B_shared[BLOCKSIZE * BLOCKSIZE];
    ```
    
    - è®¡ç®—â€æ¯ä¸ªblockå†…éƒ¨â€ï¼Œå½“å‰threadæ­£åœ¨è®¿é—®çš„æ•°æ®ä½ç½®
    
    ```jsx
        // the inner row & col that we're accessing in this thread
        const uint thread_row = threadIdx.x / BLOCKSIZE;
        const uint thread_col = threadIdx.x % BLOCKSIZE;
    ```
    
    - ä¾æ®å½“å‰blockçš„idxï¼Œåœ¨è¾“å…¥è¾“å‡ºæ•°æ®ï¼ˆçŸ©é˜µABCï¼‰ä¸­å¯»å€å¯¹åº”çš„ä½ç½®ï¼š
    
    ```jsx
        // the output block that we want to compute in this threadblock
        const uint c_row = blockIdx.x;
        const uint c_col = blockIdx.y;
        
        // advance pointers to the starting positions
        A += c_row * BLOCKSIZE * K; // A \in [M,K]
        B += c_col * BLOCKSIZE;     // B \in [K,N]
        C += c_row * BLOCKSIZE * N + c_col * BLOCKSIZE;  // C \in [M,N]
    ```
    
    - åœ¨Kç»´åº¦ä¸Šè¿›è¡Œfor loop, å…¶ä¸­é—´éš”ä¸ºBlock_SIZEï¼ŒæŠŠåŸæœ¬åœ¨Kç»´åº¦ä¸Šè¿­ä»£ï¼Œæ‹†åˆ†æˆä¸¤å±‚ï¼š
        - åœ¨`K//BLOCK_SIZE`ï¼Œä»¥åŠåœ¨`BLOCK_SIZE`å†…éƒ¨çš„è¿­ä»£ ï¼ˆæ³¨æ„åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¹¶æ²¡æœ‰å¤„ç†è¶Šç•Œçš„æƒ…å†µï¼Œè€Œæ˜¯å‡è®¾äº†`K`èƒ½å¤Ÿæ•´é™¤`BLOCK_SIZE`, `256//32` ï¼‰
        
        ```jsx
        for (int i = 0; i < K; i += BLOCKSIZE)
        ```
        
        - è¯»å–å¯¹åº”å—çš„æ•°æ®
        
        ```jsx
        // load the next block of the input matrices into shared memory
        A_shared[thread_row * BLOCKSIZE + thread_col] = A[thread_row * K + thread_col];
        B_shared[thread_row * BLOCKSIZE + thread_col] = B[thread_row * N + thread_col];
        ```
        
        - åŒæ­¥  `__syncthreads();`
        - è¿›è¡Œå±€éƒ¨çš„ç´¯åŠ ï¼Œå¾ªç¯`BLOCK_SIZE`æ¬¡ï¼š
        
        ```jsx
        for (int j = 0; j < BLOCKSIZE; j++)
        {
            tmp += A_shared[thread_row * BLOCKSIZE + j] * B_shared[j * BLOCKSIZE + thread_col];
        }
        ```
        
        - åŒæ­¥  `__syncthreads();`
        - è°ƒæ•´æŒ‡é’ˆçš„ä½ç½®ï¼ˆå…¶å®ä¹Ÿå¯ä»¥ç”¨å¾ªç¯å˜é‡iï¼‰
        
        ```jsx
        // advance the pointers
        A += BLOCKSIZE;
        B += BLOCKSIZE * N;
        ```
        
        ---
        
        - (*) å½“ä¸€ä¸ªBlockå†…éƒ¨çš„threadæ‰€æ‰§è¡Œçš„å†…å®¹å¹¶ä¸æ˜¯å®Œå…¨å¯ä»¥å¹¶è¡Œæ—¶ï¼Œéœ€è¦åœ¨Blockå†…éƒ¨è¿›è¡Œ**â€åŒæ­¥â€œ**ï¼Œä½¿ç”¨ `__syncthreads()`ï¼Œåœ¨è¯¥ä¾‹å­ä¸­ï¼š
            - ï¼ˆ1ï¼‰è¯»å–æ•°æ®åˆ°shared memoryä¹‹åéœ€è¦åŒæ­¥
            - ï¼ˆ2ï¼‰åˆ©ç”¨shared memoryè¿›è¡Œå±€éƒ¨partial sumä¹‹åä¹Ÿéœ€è¦åŒæ­¥ã€‚
- **å…¨æµç¨‹çš„ç¤ºæ„å›¾ï¼š**

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled.jpeg)

- ä¾‹ï¼šA100çš„å†…å­˜å±‚æ¬¡ç»“æ„å›¾ç¤º

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%206.png)

### 5.2  ä¼˜åŒ–çŸ©é˜µä¹˜æ³•Example-2ï¼ˆä½¿ç”¨1-D Thread TIleï¼‰ï¼š

- ä¸Šé¢ä¾‹å­ä¸­ç”¨ä¸€ä¸ªthreadæ¥å¯¹åº”[M,N]ä¸­çš„è¾“å‡ºæ•°æ®ï¼Œå¹¶è¡Œåº¦å¤ªé«˜ï¼Œå¯¼è‡´â€ç®—æœ¯å¼ºåº¦â€œï¼ˆè®¡ç®—é‡/å†…å­˜æ¬è¿é‡ï¼‰è¿‡ä½ã€‚
- è®©ä¸€ä¸ªThreadè®¡ç®—æ›´å¤šæ•°æ®å¯ä»¥å¢åŠ Arithmetic Intensityçš„åŸå› ï¼š
    - è‡³å°‘è¿™ä¸ªä¾‹å­ä¸­ï¼Œè¯»åˆ°çš„ä¸€äº›æ•°æ®å¯ä»¥åœ¨è®¡ç®—ä¸­å¤ç”¨*ï¼ˆè¯»2*7ä¸ªæ•°æ®ï¼Œå¯ä»¥è®¡ç®—å¾—åˆ°4ä¸ªè€Œä¸æ˜¯2ä¸ªå€¼ï¼‰

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%207.png)

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%208.png)

- **1ç»´Thread Tileå¹¶è¡Œï¼š**å°†Thread Blockä¸­çš„Threadä¸€ç»´åˆ’åˆ†ï¼Œæ¯ä¸ªThreadè´Ÿè´£ä¸€éƒ¨åˆ†è€Œä¸æ˜¯ä¸€ä¸ªæ•°æ®
    - å¯ä»¥å‡å°‘Blockçš„ä¸ªæ•°ï¼Œå‡å°‘Blockçš„åŒæ­¥å¼€é”€
- **Code Read:**
    - æ–°å¢ä¸€ä¸ªå†…å¾ªç¯ï¼Œæ¯ä¸ªThreadè®¡ç®—å¤šä¸ªæ¡ç›®ï¼Œç¼“å­˜ä¸å†æ˜¯ä¸€ä¸ª`BLOCK_SIZE x BLOCK_SIZE`çš„ï¼Œè€Œæ˜¯ `BMxBK+BNxBK`
    - æœ¬æ¥åœ¨æœ€å†…å±‚çš„åªæœ‰`BK`ç»´åº¦çš„å¾ªç¯ï¼Œæ¯ä¸ªthreadå‡ºä¸€ä¸ªæ•°å­—ï¼Œç°åœ¨æ”¹ä¸ºäº†æ¯ä¸ªthreadRow(å¯¹åº”Açš„ä¸€æ¡æ•°æ®)ç®—å‡º`1xTM` ä¸ªæ•°å­—ï¼ˆå›¾ä¸­é˜´å½±çš„æ©™è‰²å’Œç²‰è‰²çš„ï¼‰
    - ç”¨  `for (uint res_idx = 0; res_idx < TM; res_idx++)`  çš„ä¸€é‡å¾ªç¯
    - æŸä¸ªThreadå±€éƒ¨çš„å€¼å­˜å‚¨è¦å­˜åœ¨register_fileé‡Œï¼Œä¸ç„¶é»˜è®¤å†™åˆ°Shared/Global Memoryä¸­
    
    ```jsx
     float thread_results[TM] = {0.0};
    ```
    
- åœ¨kernelå®šä¹‰çš„æ—¶å€™å°±æŒ‡å®šäº†æ¨¡æ¿å‚æ•°ï¼Œè°ƒç”¨çš„æ—¶å€™ä¹Ÿè¾“å…¥è¿›å»ï¼š

```jsx
// ---------- Definition --------
template <const int BM, const int BN, const int BK, const int TM>
__global__ void sgemm_blocktiling_1d_kernel(float *A, float *B, float *C, int M, int N, int K)

// ------------ Actual Calling ------------
    sgemm_blocktiling_1d_kernel<BM, BN, BK, TM>
        <<<grid_size, block_size>>>(A, B, C, m, n, k);
```

- å…¨æµç¨‹å›¾ç¤ºï¼š

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%209.png)

- ä¸€äº›å…¶ä»–å…³äºGEMMä¼˜åŒ–çš„æ–‡æ¡£ï¼š
    - [CUDA çŸ©é˜µä¹˜æ³•ç»ˆæä¼˜åŒ–æŒ‡å— - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/410278370)
    - https://github.com/wangzyon/NVIDIA_SGEMM_PRACTICE

### 6.1 Reduceå®ç°Example

- `â€Reduceâ€` ï¼ˆè§„çº¦ï¼‰ä½œä¸ºä¸€ä¸ªé«˜é˜¶å‡½æ•°(high-order function)ï¼š
    - [Fold (higher-order function) - Wikipedia](https://en.wikipedia.org/wiki/Fold_(higher-order_function))
    - å°†æŸä¸ªæ•°æ®ç»“æ„recursivelyï¼Œé‡‡ç”¨ä¸€ä¸ªCombineå‡½æ•°å°†å…¶èšåˆä¸ºä¸€ä¸ªå€¼ï¼ˆä»¥ä¸‹ä¾‹å­ä¸­æˆ‘ä»¬ç”¨sumä½œä¸ºè¿™ä¸ªèšåˆï¼Œæ•°æ®ç»“æ„ä¸ºListï¼Œé€€åŒ–ä¸ºä¸€ä¸ªæœ€ç®€å•çš„arrayæ±‚å’Œçš„é—®é¢˜ï¼‰
- CPUçš„ç®€å•å®ç°ï¼š

```jsx
// reduce cpu version
int reduce(int *arr, int len) {
    int sum = 0;
    for (int i = 0; i < len; i++) {
        sum += arr[i];
    }
    return sum;
}
```

- GPUå®ç°ï¼š
    - ç¬¬ä¸€è½®è®¡ç®—ä¸­ï¼šå¥‡æ•°çº¿ç¨‹å°†è‡ªå·±çš„å€¼ç´¯åŠ åˆ°å¶æ•°çº¿ç¨‹ä¸­
    - ç¬¬äºŒè½®ï¼šï¼ˆæ¯4ä¸ªä¸€ç»„ä¸­ï¼‰ç¬¬0ä¸ªå’Œ2ä¸ªç´¯åŠ åˆ°ç¬¬0ä¸ªä¸­
    - ä¾æ¬¡ä»¥`2^N` å¾ªç¯ï¼Œç›´åˆ° `BLOCK_SIZE`  (`bdim`)
    - æ‰€æœ‰Blockç´¯åŠ 

```jsx
    // æ¯ä¸ªçº¿ç¨‹è®¡ç®— log2(bdim)-1 ä¸ªè½®å›
    // æ¯”å¦‚ bdim = 8, åˆ™æ¯ä¸ªçº¿ç¨‹è®¡ç®— 2 ä¸ªè½®å›
    for (int s = 1; s < bdim; s *= 2)
    {
        if (tid % (2 * s) == 0 && i + s < len)
        {
            sdata[tid] += sdata[tid + s];
        }
        // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ åå†è¿›è¡Œä¸‹ä¸€è½®è®¡ç®—
        __syncthreads();
    }
```

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2010.png)

### 6.2 Reduceæ”¹è¿›ï¼šäº¤é”™å¯»å€ï¼ˆInterleaved Addressingï¼‰

- ä¸Šè¿°æœ´ç´ Codeçš„å…³é”®é—®é¢˜ï¼š
    - Warp Divergent åŒä¸€ä¸ªWarpä¸­åœ¨æ‰§è¡Œä¸åŒæŒ‡ä»¤ï¼Œä¼šå¯¼è‡´éƒ¨åˆ†æŒ‡ä»¤è¢«é˜»å¡ï¼ˆä¸€ä¸ªWarpï¼ŒåŒ…å«32ä¸ªThreadï¼Œä¸€ä¸ªWarpä¸­çš„çº¿ç¨‹æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œè¦ä¹ˆæ‰§è¡Œifè¦ä¹ˆæ‰§è¡Œelseåˆ†æ”¯ï¼Œè¯¦ç»†åŸç†è§ `â€œ7. CUDAæ‰§è¡Œæ¨¡å‹â€`ï¼‰
    - å–æ¨¡æ“ä½œ `%` æœ¬èº«å¼€é”€å¤§ï¼š
        - åŒ…å«é™¤æ³•ï¼Œé™¤æ³•**è¾ƒéš¾å¹¶è¡ŒåŒ–**ï¼Œå…¶ä»–ä¸€äº›çº¿ç¨‹ç­‰å¾…é™¤æ³•çº¿ç¨‹ï¼Œå¯¼è‡´å¹¶è¡Œåº¦ä¸‹é™
        - å–æ¨¡æ“ä½œä¼´éšç€å†…å­˜è®¿é—®ï¼Œä¼šå¯¼è‡´ä¸è§„åˆ™çš„å†…å­˜è®¿é—®æ¨¡å¼
- ä¹‹å‰çš„ä»£ç é‡Œé¢æˆ‘ä»¬æ˜¯åŸºäºçº¿ç¨‹çš„ id æ¥è¿›è¡Œå¯»å€çš„ï¼Œå¶æ•°çº¿ç¨‹çš„è¡Œä¸ºå’Œå¥‡æ•°çº¿ç¨‹çš„è¡Œä¸ºæ˜¯ä¸ä¸€æ ·çš„ï¼Œå¯¼è‡´äº†**ä¸€åŠçš„çº¿ç¨‹è¢«é˜»å¡ï¼Œæ²¡æœ‰ç”¨ä¸Š**ã€‚

```jsx
// ä¸ä½¿ç”¨äº¤é”™å¯»å€
for (int s = 1; s < bdim; s *= 2)
{
    if (tid % (2 * s) == 0 && i + s < len)
    {
        sdata[tid] += sdata[tid + s];
    }
    // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ åå†è¿›è¡Œä¸‹ä¸€è½®è®¡ç®—
    __syncthreads();
}

// ä½¿ç”¨äº¤é”™å¯»å€
for (int s = 1; s < bdim; s *= 2)
{
    int index = 2 * s * tid;
    if ((index + s < bdim) && (bdim * bid + s < len))
    {
        sdata[index] += sdata[index + s];
    }
}
```

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2011.png)

ï¼ˆå§‹ç»ˆåªæœ‰å¶æ•°çš„Threadåœ¨è¿›è¡Œè®¡ç®—ï¼Œè€Œå¥‡æ•°çº¿ç¨‹é˜»å¡ï¼‰

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2012.png)

å½“BLOCK_SIZEä¸º256æ—¶ï¼Œä¸€ä¸ªBlockåŒ…å«äº†8ä¸ªWarpï¼ˆ32ä¸ªçº¿ç¨‹ç»„ï¼ŒWarp_idxå¦‚å›¾ä¸­åºå·æ‰€ç¤ºï¼‰ï¼ˆå‰ä¸‰æ¬¡è¿­ä»£çš„æ—¶å€™ä¸å­˜åœ¨Warp Divergentï¼Œç¬¬ä¸‰æ¬¡è¿­ä»£ä¾ç„¶èƒ½ç”¨æ»¡ä¸€ä¸ªWarpï¼‰ï¼Œ**â€è®©è¿ç»­çš„çº¿ç¨‹å°½é‡ä¿æŒä¸€æ ·çš„è¡Œä¸ºâ€œ**

- ï¼ˆï¼Ÿï¼‰ Warp Divergentå¯¼è‡´æ…¢çš„æœ¬è´¨åŸå› ï¼Ÿåœ¨ä¸Šè¿°çš„ä¾‹å­ä¸­ï¼Œ ä»ç„¶æœ‰ä¸€åŠçš„çº¿ç¨‹ä¼šæ˜¯IDLEçš„ï¼Ÿ

### 6.2 Reduceæ”¹è¿›ï¼šè§£å†³Bank Conflict

- Bankï¼šShared Memoryçš„æœ€å°å•å…ƒï¼Œå¦‚æœå¤šä¸ªthreadè®¿é—®åŒä¸€ä¸ªbankï¼Œä»–ä»¬çš„è®¿é—®æ˜¯ä¸²è¡ŒåŒ–çš„ï¼Œä»è€Œå¯¼è‡´é˜»å¡ã€‚
    - å…±äº«å†…å­˜é€»è¾‘ä¸Šè¢«åˆ†ä¸º32ä¸ªbank
    - é¿å…å†²çªçš„å…¸å‹æ–¹æ³•ï¼šæ”¹å˜æ•°æ®å¸ƒå±€ï¼Œä½¿ç”¨paddingï¼ŒshuffleæŒ‡ä»¤ç­‰ã€‚
- ä¸Šè¿°äº¤å‰å¯»å€çš„æ–¹å¼å¸¦æ¥äº†æ–°çš„é—®é¢˜â€bank conflictâ€œï¼šå½“åŒä¸€ä¸ªwarpä¸­å¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€ä¸ªbankçš„æ—¶å€™ï¼Œå‡ºç°å†²çªã€‚
    - **å…·ä½“çš„ï¼š**0å·çº¿ç¨‹åŠ è½½shared memoryä¸­çš„0,1,å†™å›0ï¼›ç¬¬16å·çº¿ç¨‹åŠ è½½32,33ï¼Œå†™å›32ã€‚åœ¨è¿™ä¸€ä¸ªwarpå†…è®¿é—®äº†ä¸€ä¸ªbankçš„ä¸åŒåœ°å€ï¼ˆ1å’Œ33å·ä½ç½®ï¼‰
        
        ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2013.png)
        
    - **è§£å†³æ–¹æ¡ˆï¼š**è®©ä¸€ä¸ªWarpå†…çš„çº¿ç¨‹ä¸æ˜¯åŒä¸€ä¸ªBank
        
        ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2014.png)
        
- å¦‚ä½•æ£€æµ‹åˆ°Bank Conflict:
    - bank conflictå¯ä»¥åœ¨nvprofå·¥å…·ä¸­è¿›è¡ŒæŸ¥çœ‹ï¼ˆå¯¹nsysä¼¼ä¹ä¸é€‚ç”¨ï¼‰ï¼š
        
        ```jsx
        nsys nvprof --events shared_st_bank_conflict ./reduce_interleaved_addressing
        
        // The --events shared_st_bank_conflict switch is ignored by nsys.
        
        ```
        
    - åªèƒ½ä½¿ç”¨Nsight Computeäº†
        
        ```jsx
        sudo ncu --metrics l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_st.sum  ./reduce_interleaved_addressing
        ```
        

## 7. CUDAæ‰§è¡Œæ¨¡å‹

- SMï¼ˆStream Processorï¼‰æ¶æ„å›¾ï¼š
    - CUDA Core è®¡ç®—æ ¸å¿ƒ
    - å…±äº«å†…å­˜/ä¸€çº§ç¼“å­˜ï¼ˆShared Memory/L1 Cacheï¼‰
    - å¯„å­˜å™¨æ–‡ä»¶ï¼ˆRegister Fileï¼‰
    - åŠ è½½å­˜å‚¨å•å…ƒï¼ˆLoad/Store Unitï¼‰
    - ç‰¹æ®ŠåŠŸèƒ½å•å…ƒï¼ˆSFU, Special Function Unitï¼‰
    - ç°æˆæŸè°ƒåº¦å™¨ï¼ˆWarp Schedulerï¼‰

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2015.png)

- è½¯ç¡¬ä»¶å¯¹åº”æ¨¡å‹

![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2016.png)

- è½¯ç¡¬ä»¶å¯¹åº”åˆ†é…å…³ç³»ï¼š
    - å½“ä¸€ä¸ªThread Blockè¢«åˆ†é…åˆ°æŸä¸ªSMä¸­æ‰§è¡Œä¹‹åï¼Œä»–å°±åªèƒ½åœ¨è¿™ä¸ªSMä¸­æ‰§è¡Œäº†
    - çº¿ç¨‹æŸï¼ˆWarpï¼‰ï¼šå½“å°†blockåˆ†é…ç»™SMä¹‹åï¼ŒSMå°†è¯¥blockåˆ’åˆ†ä¸ºå¤šä¸ªwarpï¼Œä»¥ä¾›ç¡¬ä»¶è°ƒåº¦ã€‚
    - çº¿ç¨‹å—Blockå®è´¨ä¸Šæ˜¯é€»è¾‘äº§ç‰©ï¼Œåœ¨ç¡¬ä»¶ä¸­æ˜¯ä¸å­˜åœ¨çš„ï¼›è€Œå†…å­˜æ˜¯ä¸€ç»´çº¿æ€§å­˜åœ¨çš„ï¼Œblockçš„å¼•å…¥æ˜¯ä¸ºè®©ç¨‹åºæ›´å®¹æ˜“è¢«ç†è§£ï¼Œå› ä¸ºç»å¸¸å¤„ç†çš„æ˜¯å›¾åƒï¼Œå› æ­¤ç”¨3ç»´çš„Blockä¼šæ›´å®¹æ˜“ç†è§£ã€‚
- CUDAå¯¹Threadçš„ç®¡ç†æ–¹å¼ï¼š**SIMT**ï¼ˆå•æŒ‡ä»¤ï¼Œå¤šçº¿ç¨‹ï¼‰ï¼Œåœ¨ä¸€ä¸ªçº¿ç¨‹æŸï¼ˆWarpï¼Œ32ä¸ªthreadä¸ºä¸€ç»„ï¼‰ä¸­çš„æ‰€æœ‰threadåŒæ—¶æ‰§è¡Œç›¸åŒçš„æŒ‡å®šï¼Œä½†æ˜¯æ¯ä¸ªçº¿ç¨‹å¤„ç†çš„æ•°æ®ä¸åŒï¼Œæœ‰è‡ªå·±çš„æŒ‡ä»¤åœ°å€è®¡æ•°å™¨å’Œå¯„å­˜å™¨ã€‚
- Magic Number: 32, ç”±ç¡¬ä»¶å†³å®šçš„ã€‚å®è´¨æ˜¯SMç”¨SIMDæ–¹å¼å¤„ç†çº¿ç¨‹æ—¶çš„**å·¥ä½œç²’åº¦**ã€‚
- **çº¿ç¨‹åŒæ­¥æœºåˆ¶ï¼š**å½“å¤šä¸ªçº¿ç¨‹ä»¥æœªå®šä¹‰é¡ºåºè®¿é—®åŒä¸€æ•°æ®ï¼Œå¯èƒ½ä¼šé€ æˆä¸å¯é¢„æµ‹çš„è¡Œä¸ºï¼ŒCUDAå†…éƒ¨æœ‰åŒæ­¥æœºåˆ¶ï¼Œä½†æ˜¯Blockä¹‹é—´çš„åŒæ­¥ï¼Œéœ€è¦æ˜¾ç¤ºçš„åˆ©ç”¨åŒæ­¥åŸè¯­ `__synthreads()`
- **èµ„æºåˆ†é…é—®é¢˜ï¼š**
    - æ¯ä¸ªSMä¸Šæœ‰32ä½çš„å¯„å­˜å™¨ï¼Œä¸ä¸€å®šé‡çš„å…±äº«å†…å­˜æ¥åˆ†é…ã€‚æ¯ä¸ªç°æˆéœ€è¦çš„å¯„å­˜å™¨è¶Šå¤šï¼Œé‚£ä¹ˆSMä¸Šå¯è¿è¡Œï¼ˆæ´»è·ƒçš„ï¼‰Threadå°±è¶Šå°‘ã€‚
- **æœ€å¤§åŒ–æ´»è·ƒWarpæ•°ï¼š**ä¸ºäº†æ›´é«˜çš„åˆ©ç”¨ç‡
    - å½“è®¡ç®—èµ„æºè¢«åˆ†é…ç»™æŸä¸ªBlockæ—¶ï¼Œè¯¥blockä¸­æ‰€åŒ…å«çš„warpå°±æ˜¯æ´»è·ƒçš„warpã€‚ï¼ˆï¼Ÿï¼‰è¿™æ ·warp scheduleræ‰æœ‰ç©ºé—´è¿›è¡Œscheduleã€‚
- éšè—å»¶è¿Ÿï¼šå°½é‡ä¿æŒåŒä¸€æ—¶é—´ï¼Œå³ä½¿æœ‰éƒ¨åˆ†çº¿ç¨‹è¢«é˜»å¡ï¼Œè¿˜æœ‰åˆ«çš„çº¿ç¨‹å¯ä»¥è¢«æ‰§è¡Œ
    
    ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2017.png)
    
    - æŒ‡ä»¤çš„å»¶è¿Ÿåˆ†ä¸ºä¸¤ç§ï¼š**ç®—æœ¯å»¶è¿Ÿ**ï¼ˆä¸€ä¸ªç®—æœ¯æ“ä½œä»å¼€å§‹ï¼Œåˆ°äº§ç”Ÿç»“æœä¹‹é—´çš„æ—¶é—´ï¼‰ & **å†…å­˜å»¶è¿Ÿ**ï¼ˆäº§ç”Ÿå†…å­˜è®¿é—®çš„æ—¶å€™ï¼Œè®¡ç®—å•å…ƒè¦ç­‰æ•°æ®ä»å†…å­˜æ‹¿åˆ°å¯„å­˜å™¨ï¼‰
    - åªè¦çº¿ç¨‹æŸè¶³å¤Ÿå¤šï¼Œé‚£ä¹ˆå°±å¯ä»¥éšè—æ›´å¤šçš„å»¶è¿Ÿã€‚**é‚£ä¹ˆè‡³å°‘éœ€è¦å¤šå°‘çº¿ç¨‹ï¼Œçº¿ç¨‹æŸæ¥ä¿è¯æœ€å°åŒ–å»¶è¿Ÿå‘¢ï¼Ÿ  åˆ©ç‰¹å°”æ³•åˆ™ï¼ˆLittleâ€™s Lawï¼‰**
    
    ```jsx
    æ‰€éœ€çº¿ç¨‹æŸ=å»¶è¿ŸÃ—ååé‡
    ```
    
    - å‡è®¾Kernelä¸­æŸæ¡æŒ‡ä»¤çš„å»¶è¿Ÿæ˜¯5ä¸ªå‘¨æœŸï¼Œä¸ºäº†ä¿æŒåœ¨å‘¨æœŸå†…æ‰§è¡Œ6ä¸ªwarpçš„ååç‡ï¼Œéœ€è¦30ä¸ªæœªå®Œæˆçš„Warpã€‚
- çº¿ç¨‹æŸè°ƒåº¦ï¼ˆWarp Schedulerï¼‰å’ŒæŒ‡ä»¤è°ƒåº¦å•å…ƒï¼ˆInstruction Dispatch Unitï¼‰
    - æ¯ä¸ªSMæœ‰è‹¥å¹²ä¸ªï¼ˆä»¥2ä¸ªä¸ºä¾‹ï¼‰
    - 2ä¸ªWarp Scheduleré€‰æ‹©2ä¸ªWarpï¼Œç”¨æŒ‡ä»¤è°ƒåº¦å™¨å­˜å‚¨ä¸¤ä¸ªWarpæ‰€å¯¹åº”çš„æŒ‡ä»¤
    
    ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2018.png)
    
    - æ¯ä¸ªwarpåœ¨åŒä¸€æ—¶åˆ»æ‰§è¡Œä¸€ä¸ªæŒ‡ä»¤ï¼Œè€ŒåŒä¸€ä¸ªblockä¹‹é—´åˆ‡æ¢warpæ˜¯æ²¡æœ‰æ—¶é—´æ¶ˆè€—çš„overheadçš„ï¼Œå› æ­¤ï¼ŒGPUæ”¯æŒâ€å¹¶å‘æ‰§è¡ŒKernelâ€œï¼Œå¯ä»¥å¹¶å‘ä¸€äº›å°çš„kernelæ¥å……åˆ†åˆ©ç”¨GPUã€‚
    
    ![Untitled](%5BCUDA%20Learning%5D%20fe71681b7adc4709871b2af69c765163/Untitled%2019.png)
    

---

# Digest of: Nvidiaâ€™s CUDA Basics

> ç¬”è®°ï¼š[CUDA.pdf (iitk.ac.in)](https://www.cse.iitk.ac.in/users/biswap/CASS18/CUDA.pdf)
> 
- Compute Compatibility: ç”¨æ¥æè¿°ç‰¹å®šå¹³å°æ¶æ„æ‰€æ”¯æŒçš„Feature
- å½“å‰Kernelèƒ½æŒæ¡å¤šå°‘èµ„æºä¼˜å†…å»ºå˜é‡ï¼š
    - blockDim, GridDimæ‰€å®šä¹‰
- Textures: æ˜¯ä¸€ä¸ªReadOnlyçš„ï¼Œç‰©ç†ä¸Šçš„Cacheç»“æ„

# [Application] å°†CUDA Codeæ‰“åŒ…ä¸ºPython Extensionçš„å½¢å¼

> ä¸»è¦å‚è€ƒäº†PyTorchå®˜æ–¹çš„Tutorialï¼šhttps://pytorch.org/tutorials/advanced/cpp_extension.html
> 

> å®˜æ–¹çš„æ ·ä¾‹ä¸ºï¼š[pytorch/extension-cpp: C++ extensions in PyTorch (github.com)](https://github.com/pytorch/extension-cpp/tree/master)
> 

> ï¼ˆæœ¬åœ°åŒ–ï¼‰æ ·ä¾‹ä¸º[A-suozhang/diffuser-dev at guyue/mixdq_demo (github.com)](https://github.com/A-suozhang/diffuser-dev/tree/guyue/mixdq_demo)
> 

> æ­¤å‰çš„ç¬”è®°ï¼š[â€â â â€Œâ€¬â€¬â€¬â€Œâ€¬â€Œâ€¬â€¬â ï»¿â€â€‹â€¬â€¬[MixDQ åŠ é€ŸDemo] - é£ä¹¦äº‘æ–‡æ¡£ (feishu.cn)](https://infinigence.feishu.cn/wiki/BtZLwOyYniUg80k1jPTcUANrn1f)
> 

### Installation

- åœ¨æœ¬åœ°çš„`setup.py`ä¸­å®šä¹‰äº†å¦‚ä½•ç¼–è¯‘å‡ºwheelå¹¶å®‰è£…ï¼Œé€šè¿‡ `pip install .`  è¿›è¡Œå®‰è£…
    - `get_extension()` ä¸­æè¿°äº†ä»ä»€ä¹ˆè·¯å¾„fetchæ‰€æœ‰çš„ccä»¥åŠcuæ–‡ä»¶ï¼Œç¼–è¯‘argsï¼Œä»¥åŠåŒ…çš„åå­—ä¸ç‰ˆæœ¬è¯´æ˜
        - åŒ…å«äº†extension_dirä¸­çš„cc,cpp,cuæ–‡ä»¶
        - åŒ…å«äº†cutlassçš„headerå’Œtoolsæ–‡ä»¶
        - æœ€åpackæˆä¸€ä¸ª`CUDAExtention`ç±»(`torch.utils.cpp_extension`) ä¸­åŒ…å«çš„æ–‡ä»¶è¾“å…¥ç»™setupå‡½æ•°
    - å®Œæˆå®‰è£…åä¼šå‡ºç°ä¸€ä¸ª `${package_name}.egg-info` æ˜¯python packageå®‰è£…æ‰€äº§ç”Ÿçš„ï¼Œé‡Œé¢åŒ…å«äº†ä¸€äº›è®°å½•dependencyç­‰metadataçš„æ–‡æœ¬æ–‡ä»¶
    - `setup()` ä¸ºmainå‡½æ•°ï¼Œå…¶æè¿°çš„è¿‡ç¨‹ï¼š[Learn about Building a Python Package â€” Python Packaging Guide (pyopensci.org)](https://www.pyopensci.org/python-package-guide/package-structure-code/python-package-distribution-files-sdist-wheel.html)
        - 
    - ç¼–è¯‘å®Œæˆçš„æ–‡ä»¶åœ¨ ./build/ ä¸­ï¼ˆè‹¥å¹²MBï¼‰
        - `temp.linux-x86_64-cpython-38` ä¸­åŒ…å«äº†å„ç§.oçš„è¾“å‡ºæ–‡ä»¶
            - ä»¥åŠNinjaçš„ä¸€äº›build_log
        - `lib.linux-x86_64-cpython-38` ä¸­åŒ…å«äº†åŠ¨æ€é“¾æ¥åº“.so
- **ä¾èµ–é¡¹CUTLASS:** å†™åœ¨äº†é¡¹ç›®çš„ `.gitmodule` ä¸­ï¼Œå¯ä»¥é€šè¿‡`git clone â€”recursive`ç›´æ¥ä¸‹è½½ä¸‹æ¥ï¼Œæˆ–è€…æ‰‹åŠ¨è¿›è¡Œ `git submodule int & git submodule update`

### æ‰“åŒ…æˆPyTorchæ‰€æ”¯æŒçš„Customize Extension

- **åœ¨setup.pyä¸­é…ç½®**ï¼Œä»¥åœ¨å®‰è£…æ—¶å€™ç¼–è¯‘äº§ç”Ÿwhlï¼š

```jsx
from setuptools import setup, Extension
from torch.utils import cpp_extension

setup(name='lltm_cpp',
      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],
      cmdclass={'build_ext': cpp_extension.BuildExtension})
```

- **æ’°å†™C++ Opï¼š** (å¯¹äºæˆ‘ä»¬çš„cuda kernelæ¥è¯´ï¼Œä¸»è¦åŠ ä¸€ä¸ªC++ wrapperå³å¯)
    - ä¾‹å­ï¼š `csrc/qliner/qlinear.cc`

```jsx
#include <torch/extension.h>

#include <iostream>

torch::Tensor d_sigmoid(torch::Tensor z) {
  auto s = torch::sigmoid(z);
  return (1 - s) * s;
}

-----

at::Tensor 
qlinear_w8_a8_ohalf(const at::Tensor input_int8,
                    const at::Tensor weight_int8,
                    const at::Tensor weight_scale,
                    const at::Tensor input_scale,
                    const at::Tensor input_zero_point,
                    const at::Tensor weight_sum_by_input_channels,
                    const at::Tensor scale,
                    const at::Tensor bias0,
                    at::optional<const at::Tensor> bias)
{}
```

- `<torch/extension.h>`Â is the one-stop header to include all the necessary PyTorch bits to write C++ extensions
    - åŒ…å«äº†pybindå’ŒATenï¼ˆPytorchçš„ä¸»è¦tensor computationï¼‰æ‰€éœ€è¦çš„headers
- **Binding to Python**

```
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
  m.def("forward", &lltm_forward, "LLTM forward");
  m.def("backward", &lltm_backward, "LLTM backward");
}
```

- (?) æœ‰ä¸€ä¸ªsectionæ˜¯**using accessor**ï¼Œä¼¼ä¹æ˜¯ä¸€ä¸ªæ›´é«˜ç»´åº¦çš„ç±»Tensorå°è£…ï¼Œè€Œä¸æ˜¯ç›´æ¥

### mixdq_extensionä»£ç 

> é™¤äº†csrc/ä¸­ï¼Œéƒ½æ˜¯python codeï¼Œåœ¨`ops/`å’Œ`nn/`æ–‡ä»¶å¤¹ä¸­æ·»åŠ äº†ç©ºç™½çš„`__init__.py`ä»¥è¢«è¯†åˆ«ä¸ºpackageå¼•ç”¨ï¼Œä¸»ä½“å†…å®¹å…¶å®å°±æ˜¯teståŠŸèƒ½æ€§ã€‚
> 
- opsï¼š
    - [`quant.py`](http://quant.py) ï¼šå¯¹æ¯”äº†`torch.quantizer_per_tensor` ä¸ `mixdq_extension._C.quantize_per_tensor_to_int8`  çš„ç»“æœï¼ˆåœ¨é»˜è®¤ï¼Œä»¥åŠcuda_graphçš„settingä¸‹ï¼‰
    - `qlinear.py`ï¼šæµ‹è¯•äº†torchçš„å®ç°ä¸`qlinear = mixdq_extension._C.qlinear_w8_a8_ohalf`
    - [`qconv2d.py`](http://qconv2d.py)ï¼šæµ‹è¯•äº† `mixdq_extension._C.qconv2d_w8_a8_ohalf` å’Œ F.conv2d
- nn: (å…¼å®¹pytorch quantizationçš„è¿‡ç¨‹ï¼Œç¬¦åˆä»¿çœŸä»£ç )
    - `utils.py`: å®šä¹‰äº†QParamsï¼Œint4çš„å„ç§å¤„ç†
        - conv2d_on_quantized_data ï¼ˆDepreactedï¼‰
            - conv_cutlass
        - linear_on_quantized_dataï¼ˆDeprecatedï¼‰
            - gemm_cutlass
    - `Conv2d.py/Linear.py`: QuantizedConv2d
        - `from_float()`
        - `forward()` è°ƒç”¨äº†qconv2d
        - `forward_callback()` ä»ç„¶ç”¨F.conv2d
    - `quantizer_dequantizer.py`  ï¼ˆï¼Ÿæ˜¯å¦æœ‰è¢«ç”¨åˆ°ï¼‰
- csrcï¼š
    - 

### å¤–å›´Python Code

### Qs

- [ ]  å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¤§æ¦‚ä¸ä¼šç”¨pythonè°ƒç”¨æ¥å¯¹æ¯”referenceçš„å—ï¼Ÿ
    - opsé‡Œé¢çš„codeæ˜¯è°ƒç”¨çš„`mixdq_extension._C.qconv2d_w8_a8_ohalf` æ¥æ“ä½œï¼Œå¦‚æœè¿™æ ·çš„è¯æ¯æ¬¡ä¿®æ”¹codeéƒ½éœ€è¦è·‘ä¸€ä¸ªå®Œæ•´çš„ç¼–è¯‘è¿‡ç¨‹ï¼ˆæ˜¯ä¸æ˜¯æœ‰ç‚¹è´¹åŠ²äº†ï¼‰
    - debugçš„æ—¶å€™ç›´æ¥å†™ä¸ªmainå‡½æ•°nvccå•ä¸ªæ–‡ä»¶çš„å—
- [ ]  profilingå·¥å…·ï¼Ÿ
- [x]  conv2d_on_quantized_data æ˜¯åœ¨å“ªé‡Œè¢«ç”¨åˆ°ï¼Ÿè¿˜æ˜¯åªæ˜¯testç”¨ï¼Ÿ

# References:

- [ğŸŒŸ]ä¸­æ–‡çš„ææ–™ï¼ŒPaddleJiTLabï¼š[CUDATutorial | Notebook (keter.top)](https://cuda.keter.top/)
    - https://github.com/PaddleJitLab/CUDATutorial
    - æ·±åº¦å­¦ä¹ åŸºç¡€çŸ¥è¯†ç›®å½• | Notebook (keter.top) æ ¼å¼å‚è€ƒäº†è¿™ä¸ªåšå®¢ï¼Œå‰ç«¯å¾ˆç¾è§‚
        - [CUDAæ‰§è¡Œæ¨¡å‹æ¦‚è¿° | Notebook (keter.top)](https://space.keter.top/docs/high_performance/CUDA%E7%BC%96%E7%A8%8B/CUDA%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0)
- https://infinigence.feishu.cn/wiki/BtZLwOyYniUg80k1jPTcUANrn1f MixDQ Demoçš„æ„å»ºè¿‡ç¨‹
- [Tutorial 01: Say Hello to CUDA - CUDA Tutorial (cuda-tutorial.readthedocs.io)](https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/#:~:text=Tutorial%2001%3A%20Say%20Hello%20to%20CUDA%201%20Introduction,...%207%20Wrap%20up%20...%208%20Acknowledgments%20)
    - https://developer.nvidia.com/blog/even-easier-introduction-cuda/ çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨ReadTheDocæ”¹å†™äº†ä¸¤ä¸ªç…è›‹çš„Example
- https://github.com/RussWong/CUDATutorial ä¸€ä¸ªå›½äººç»™äº†ä¸€ç³»åˆ—NNç›¸å…³çš„ä¾‹å­ï¼Œæœ€ååŒ…å«äº† fused kernel
- [CUDA C++ Programming Guide (nvidia.com)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- A simple short Slides: [CUDA.pdf (iitk.ac.in)](https://www.cse.iitk.ac.in/users/biswap/CASS18/CUDA.pdf)
- PDF on Github: ã€ŠCUDA Programming: A Developerâ€™s Guideã€‹ ä¹¦ç±
    - [cudaLearningMaterials_2/CUDAå¹¶è¡Œç¨‹åºè®¾è®¡-GPUç¼–ç¨‹æŒ‡å—-é«˜æ¸…æ‰«æ-ä¸­è‹±æ–‡/CUDA Programming A Developer's Guide to Parallel Computing with GPUs.pdf at master Â· SeventhBlue/cudaLearningMaterials_2 (github.com)](https://github.com/SeventhBlue/cudaLearningMaterials_2/blob/master/CUDA%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1-GPU%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-%E9%AB%98%E6%B8%85%E6%89%AB%E6%8F%8F-%E4%B8%AD%E8%8B%B1%E6%96%87/CUDA%20Programming%20A%20Developer's%20Guide%20to%20Parallel%20Computing%20with%20GPUs.pdf)
- çŸ¥ä¹Example: [CUDA ç¼–ç¨‹å°ç»ƒä¹ ï¼ˆç›®å½•ï¼‰ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/365904031)
- Nvidia Developer Blogsçš„å¯¹åº”Codeï¼š[code-samples/posts at master Â· NVIDIA-developer-blog/code-samples (github.com)](https://github.com/NVIDIA-developer-blog/code-samples/tree/master/posts)
    - ä¸€äº›æ–°Featureçš„ä»‹ç»ï¼š[Programming Tensor Cores in CUDA 9 | NVIDIA Technical Blog](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)
- ä¸€ç³»åˆ—åº“çš„ç®€ä»‹ï¼š[cuBLAS - ä¸Šæµ·äº¤å¤§è¶…ç®—å¹³å°ç”¨æˆ·æ‰‹å†Œ Documentation (sjtu.edu.cn)](https://docs.hpc.sjtu.edu.cn/app/compilers_and_languages/cublas.html)